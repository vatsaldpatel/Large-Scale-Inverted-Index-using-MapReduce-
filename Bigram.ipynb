{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c12775b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do an initial test of Spark to make sure it works.\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec45bc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a9a45b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "path = 'data2/data/devdata/'\n",
    "files = os.listdir(path)\n",
    "files = [path+f for f in files]\n",
    "#print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ebe840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all \\t from the files\n",
    "for f in files:\n",
    "    #print(f)\n",
    "    lines = ''\n",
    "    with open(f, 'r', encoding=\"utf8\") as file:\n",
    "        lines = file.readline()\n",
    "        #print(lines)\n",
    "        lines = re.sub(r'\\t', ' ', lines)\n",
    "        lines = lines.split()[0] + \"\\t\" + ' '.join(lines.split()[1:])\n",
    "        file.close()\n",
    "    with open(f, 'w', encoding=\"utf8\") as file:\n",
    "        file.write(lines)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15501989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode, col, concat_ws, collect_list\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"InvertedIndex\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9552328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://VatsalPatel:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>InvertedIndex</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x263fc52b040>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14fca409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read successfully\n",
      "                                                text\n",
      "0  computer science 5722018235: 1 5722018508: 1 5...\n",
      "1  information retrieval 5722018235: 1 5722018508...\n",
      "2           bruce willis 5722018235: 4 5722018301: 3\n",
      "3  power politics 5722018235: 3 5722018508: 1 572...\n",
      "4  los angeles 5722018235: 6 5722018508: 13 57220...\n"
     ]
    }
   ],
   "source": [
    "# Specify the bigram words\n",
    "bigram_words = [\"computer science\", \"information retrieval\", \"power politics\", \"los angeles\", \"bruce willis\"]\n",
    "\n",
    "# Read data\n",
    "data = spark.read.text(files[:]).rdd.map(lambda x: x.value)\n",
    "print(\"Data read successfully\")\n",
    "\n",
    "# Define a function to clean and tokenize the text\n",
    "def clean_and_tokenize(text):\n",
    "    # Replacing \\t, special characters, and numerals with space\n",
    "    processed_text = re.sub(r'\\t', ' ', text)    \n",
    "    processed_text = re.sub(r'[^a-zA-Z]+', ' ', processed_text)\n",
    "\n",
    "    processed_text = processed_text.lower()\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
    "    words = processed_text.split()\n",
    "\n",
    "    return words\n",
    "\n",
    "# Define a UDF for the cleaning and tokenizing function\n",
    "clean_and_tokenize_udf = udf(clean_and_tokenize, ArrayType(StringType()))\n",
    "\n",
    "# Define a UDF to extract bigrams\n",
    "def extract_bigrams(words):\n",
    "    return [\" \".join(bigram) for bigram in zip(words[:-1], words[1:])]\n",
    "\n",
    "extract_bigrams_udf = udf(extract_bigrams, ArrayType(StringType()))\n",
    "\n",
    "# Split the data into docId and text\n",
    "split_data = data.map(lambda x: x.split('\\t')).map(lambda x: (x[0], x[1]))\n",
    "\n",
    "# Creating a DataFrame with the defined schema\n",
    "schema = StructType([StructField(\"docId\", StringType(), True), StructField(\"text\", StringType(), True)])\n",
    "df = spark.createDataFrame(split_data, schema=schema)\n",
    "\n",
    "# Applying the cleaning and tokenizing UDF to the 'text' column\n",
    "df = df.withColumn(\"words\", clean_and_tokenize_udf(df[\"text\"]))\n",
    "\n",
    "# Applying the bigram extraction UDF to the 'words' column\n",
    "df = df.withColumn(\"bigrams\", extract_bigrams_udf(df[\"words\"]))\n",
    "df_exploded = df.select(\"docId\", \"bigrams\").withColumn(\"bigram\", explode(\"bigrams\"))\n",
    "\n",
    "# Filtering only the specified bigram words\n",
    "df_filtered = df_exploded.filter(col(\"bigram\").isin(bigram_words))\n",
    "\n",
    "# Group by bigram, docId, and count the occurrences\n",
    "inverted_index = df_filtered.groupBy(\"bigram\", \"docId\").count()\n",
    "inverted_index = inverted_index.withColumn(\"count\", col(\"count\").cast(\"string\"))\n",
    "inverted_index = inverted_index.withColumn(\"output\", concat_ws(\": \", \"docId\", \"count\"))\n",
    "\n",
    "# Group by bigram and collect list of docId: count values\n",
    "result = inverted_index.groupBy(\"bigram\").agg(collect_list(\"output\").alias(\"output_list\"))\n",
    "#print(result.count())\n",
    "#result.show(5, truncate=False)\n",
    "#print(result.describe())\n",
    "\n",
    "df_text = result.select(concat_ws(\" \", *result.columns).alias(\"text\"))\n",
    "#df_text.show(5, truncate=False)\n",
    "#print(df_text.describe())\n",
    "# df_text.write.mode(\"overwrite\").text(\"bigram_index.txt\")\n",
    "\n",
    "tmp = df_text.toPandas()\n",
    "print(tmp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e717f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "tmp.to_csv('selected_bigram_index.txt', sep=' ', index=False, header=False, quoting=3, escapechar=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a33cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01900f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
