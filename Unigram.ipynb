{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2ed1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing an initial test of Spark to make sure it works.\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f6db22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5493761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all files in the directory\n",
    "import os\n",
    "import re\n",
    "\n",
    "path = 'data2/data/fulldata/'\n",
    "files = os.listdir(path)\n",
    "files = [path+f for f in files]\n",
    "#print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ec75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all \\t from the files\n",
    "for f in files:\n",
    "    #print(f)\n",
    "    lines = ''\n",
    "    with open(f, 'r', encoding=\"utf8\") as file:\n",
    "        lines = file.readline()\n",
    "        #print(lines)\n",
    "        lines = re.sub(r'\\t', ' ', lines)\n",
    "        lines = lines.split()[0] + \"\\t\" + ' '.join(lines.split()[1:])\n",
    "        file.close()\n",
    "    with open(f, 'w', encoding=\"utf8\") as file:\n",
    "        file.write(lines)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1104fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode, col, concat_ws, collect_list\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField\n",
    "\n",
    "# Creating a Spark session\n",
    "spark = SparkSession.builder.appName(\"InvertedIndex\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6744f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://VatsalPatel:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>InvertedIndex</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23a7ea4f040>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "577c6353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read successfullly!!!\n",
      "12623\n",
      "                                                text\n",
      "0  a 5722018444:28 5722018440:44 5722018443:33 57...\n",
      "1                                    aa 5722018435:1\n",
      "2                                  aacr 5722018445:1\n",
      "3                                 aacsb 5722018483:2\n",
      "4                                  aamc 5722018479:3\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.text(files).rdd.map(lambda x: x.value)\n",
    "print(\"Data read successfullly!!!\")\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    # Replacing \\t, special characters, and numerals with space\n",
    "    processed_text = re.sub(r'\\t', ' ', text)    \n",
    "    processed_text = re.sub(r'[^a-zA-Z]+', ' ', processed_text)\n",
    "\n",
    "    processed_text = processed_text.lower()\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
    "    words = processed_text.split()\n",
    "\n",
    "    return words\n",
    "\n",
    "# Defining a UDF for the cleaning and tokenizing function\n",
    "clean_and_tokenize_udf = udf(clean_and_tokenize, ArrayType(StringType()))\n",
    "\n",
    "# Split the data into docId and text\n",
    "split_data = data.map(lambda x: x.split('\\t')).map(lambda x: (x[0], x[1]))\n",
    "\n",
    "# Creating a Spark DataFrame\n",
    "schema = StructType([StructField(\"docId\", StringType(), True), StructField(\"text\", StringType(), True)])\n",
    "df = spark.createDataFrame(split_data, schema=schema)\n",
    "\n",
    "# Applying the cleaning and tokenizing UDF to the 'text' column\n",
    "df = df.withColumn(\"words\", clean_and_tokenize_udf(df[\"text\"]))\n",
    "df_exploded = df.select(\"docId\", \"words\").withColumn(\"word\", explode(\"words\"))\n",
    "\n",
    "# Group by word, docId, and count the occurrences\n",
    "inverted_index = df_exploded.groupBy(\"word\", \"docId\").count()\n",
    "inverted_index = inverted_index.withColumn(\"count\", col(\"count\").cast(\"string\"))\n",
    "inverted_index = inverted_index.withColumn(\"output\", concat_ws(\":\", \"docId\", \"count\"))\n",
    "\n",
    "# Group by word and collect list of docId: count values\n",
    "result = inverted_index.groupBy(\"word\").agg(collect_list(\"output\").alias(\"output_list\"))\n",
    "print(result.count())\n",
    "#result.show(5, truncate=False)\n",
    "#print(result.describe())\n",
    "\n",
    "df_text = result.select(concat_ws(\" \", *result.columns).alias(\"text\"))\n",
    "#df_text.show(15, truncate=False)\n",
    "#print(df_text.describe())\n",
    "tmp = df_text.toPandas()\n",
    "print(tmp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "980b6712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "tmp.to_csv('unigram_index.txt', sep=' ', index=False, header=False, quoting=3, escapechar=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9fdc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bed35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
